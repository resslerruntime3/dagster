---
title: "Learning Dagster from Airlfow"
description: How to get started with Dagster from an Airflow background
---

# Learning Dagster from Airflow

Airflow users often come to Dagster for a variety of reasons. Some want to take advantage of our great story around testability. Others want to deploy to an orchestrator that is container native and has better support for multiple Python environments. And still others are attracted to the promise of a better abstraction: software-defined assets.

In this tutorial, we'll help you make the switch from Airflow to Dagster. Map the concepts you're familiar with in Airflow to Dagster and provide some examples of the work you did in Airflow can be done in Dagster.

By the end of this tutorial, you will:

- Understand how concepts in Airflow map onto Dagster
- Understand how to author a Dagster Pipeline similiar to a Scheduled Airflow DAG

---

## Terminology

The first thing you need to do when approaching Dagster from Airflow is to learn the terminology. While the two systems have significant differences, there are some overlapping concepts. Here’s a quick cheat sheet to help you get started.

| Airflow concept | Dagster concept | Notes |
| --- | --- | --- |
| DAG | [Job](/concepts/ops-jobs-graphs/jobs) |  |
| Task | [Op](/concepts/ops-jobs-graphs/ops) |  |
| Operator | None | Dagster uses normal Python functions instead of framework-specific Operator classes. For off-the-shelf functionality with 3rd party tools Dagster provides [integration libraries](/integrations) |
| Scheduler | [Scheduler](/concepts/partitions-schedules-sensors/schedules) |  |
| Executor | [Executor](/deployment/executors) |  |
| Instance | [Workspace](/concepts/repositories-workspaces/workspaces) | Dagster workspaces can share the same dagit UI and operate in isolation of each other. |
| SubDAGs / TaskGroups | [Graphs](/concepts/ops-jobs-graphs/graphs), [Tags](/concepts/ops-jobs-graphs/metadata-tags#tags) and [AssetGroups](/concepts/assets/software-defined-assets#grouping-assets) | Dagster provides rich, searchable metadata and tagging support well beyond what’s offered by Airflow. See the docs to learn more. |
| Hooks | [Resources](/concepts/resources) | Dagster Resources contain a superset of the functionality of hooks and have much stronger composition guarantees; see the docs to learn more. |
| Pools | [Run Coordinator](/deployment/run-coordinator) |  |
| XComs | [IO Manager](/concepts/io-management/io-managers) | IO managers are much more powerful than XComs and allow passing large datasets between jobs |
| Trigger | [LaunchPad](/concepts/dagit/dagit#launchpad) | Triggering and configuring ad-hoc runs is easier in Dagster which allows them to be initiated through the UI, graphql API or the CLI |
| Sensor | [Sensor](/concepts/partitions-schedules-sensors/sensors) |  |
| DAG Run | Job Run |  |
| Plugins/Providers | [Integrations](/integrations) |  |
| Datasets | [Software-Defined Assets](/concepts/assets/software-defined-assets) | SDAs are much more powerful and mature than datasets, include support for things like partitioning |
| Connections/Variables | [Run Config](/concepts/configuration/config-schema#run-configuration), [Configured APIs](/concepts/configuration/configured) and [Environment Variables](https://docs.dagster.io/dagster-cloud/developing-testing/environment-variables-and-secrets#dagster-cloud-environment-variables-and-secrets) (cloud only)  |  |


A Graph is a collection of Ops connected by data dependencies. if you've used the newer Airflow Task Flow API this style of programming will be familiar. At a high level instead of imperitively telling the orchestration engine when to do things with operators you will instead be focused on declaring the relationships between your data assets allowing the orchestration engine to take care of scheduling and execution. This declaritive programming model becomes most clear in the Software Defined

## Comparing an Airflow Dag to Dagster

In this guide we will rewrite an Airflow Dag as a Dagster [Job](/concepts/ops-jobs-graphs/job).

```python
from datetime import datetime, timedelta
from textwrap import dedent

# The DAG object; we'll need this to instantiate a DAG
from airflow import DAG

# Operators; we need this to operate!
from airflow.operators.bash import BashOperator
with DAG(
    "tutorial",
    # These args will get passed on to each operator
    # You can override them on a per-task basis during operator initialization
    default_args={
        "depends_on_past": False,
        "email": ["airflow@example.com"],
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    },
    description="A simple tutorial DAG",
    schedule=timedelta(days=1),
    start_date=datetime(2021, 1, 1),
    catchup=False,
    tags=["example"],
) as dag:

    # t1, t2 and t3 are examples of tasks created by instantiating operators
    t1 = BashOperator(
        task_id="print_date",
        bash_command="date",
    )

    t2 = BashOperator(
        task_id="sleep",
        depends_on_past=False,
        bash_command="sleep 5",
        retries=3,
    )
    t1.doc_md = dedent(
        """\
    #### Task Documentation
    You can document your task using the attributes `doc_md` (markdown),
    `doc` (plain text), `doc_rst`, `doc_json`, `doc_yaml` which gets
    rendered in the UI's Task Instance Details page.
    ![img](http://montcs.bloomu.edu/~bobmon/Semesters/2012-01/491/import%20soul.png)
    **Image Credit:** Randall Munroe, [XKCD](https://xkcd.com/license.html)
    """
    )

    dag.doc_md = __doc__  # providing that you have a docstring at the beginning of the DAG; OR
    dag.doc_md = """
    This is a documentation placed anywhere
    """  # otherwise, type it like this
    templated_command = dedent(
        """
    {% for i in range(5) %}
        echo "{{ ds }}"
        echo "{{ macros.ds_add(ds, 7)}}"
    {% endfor %}
    """
    )

    t3 = BashOperator(
        task_id="templated",
        depends_on_past=False,
        bash_command=templated_command,
    )

    t1 >> [t2, t3]
```

In order to rewrite this DAG in Dagster we will break it down into three parts

1. The graph of compute
2. The definition of the schedule
3. The configuration of that graph and schedule


### Defining the graph

A [Job](/concepts/ops-jobs-graphs/job) is made up of a [Graph](/concepts/ops-jobs-graphs/graphs) of [Ops](/concepts/ops-jobs-graphs/ops). If you've used the Airflow taskflow api this will feel familiar, With ops you will be focused on writing a graph with python functions as the nodes with the data dependencies between those as edges.

After rewriting the core compute of the tutorial Airflow Dag in Dagster we're left with:
```python
import time
from datetime import date, timedelta

from dagster import job, op

@op
def print_date():
    dt = date.today()
    print(dt)
    return dt

@op
def sleep(dt: date):
    time.sleep(5)

@op
def templated(dt: date):
    for i in range(5):
        print(dt)
        print(dt - timedelta(days=7))

@job
def tutorial_job():
    dt = print_date()
    sleep(dt)
    templated(dt)

@repository
def rewrite_repo():
    return [tutorial_job]
```

### Defining the schedule

next we'll add a schedule, in dagster unlike airflow schedules are defined seperately from the definition of compute


```python
schedule = ScheduleDefinition(job=tutorial_job, cron_schedule="@daily")

@repository
def rewrite_repo():
    return [tutorial_job, schedule]
```

At this point we've done a basic rewrite of the tutorial dag but have left out a few features

### Configuration
#### Retries

##### Job Level Retries
Job level retries are managed by the [run launcher](/deployment/run-retries#run-retries) you will need to enable support for them in your dagster.yaml, once you do you can define the retry count on the job.

```python
@job(tags={"dagster/max_retries": 1})
def tutorial_job():
    dt = print_date()
    sleep(dt)
    templated(dt)
```


##### Op Level Retries
In the tutorial Dag the sleep task allowed for 3 retries, to configure this in dagster you can use [Op level retry policies](/concepts/ops-jobs-graphs/op-retries)
```python
@op(
    retry_policy=RetryPolicy(
        max_retries=3
    )
)
def sleep(dt: date):
    time.sleep(5)
```

#### Tagging and Metadata

Dagster has a [tagging and metadata system](/concepts/ops-jobs-graphs/metadata-tags) that is quite a bit more powerful and integrated with the UI than what exists in Airflow.

```python
@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    dt = print_date()
    sleep(dt)
    templated(dt)
```

### The final product

```python
import time
from datetime import date, timedelta

from dagster import RetryPolicy, ScheduleDefinition, job, op, repository


@op
def print_date():
    dt = date.today()
    print(dt)
    return dt

@op(
    retry_policy=RetryPolicy(
        max_retries=3
    )
)
def sleep(dt: date):
    time.sleep(5)

@op
def templated(dt: date):
    for i in range(5):
        print(dt)
        print(dt - timedelta(days=7))

@job(tags={"dagster/max_retries": 1, "dag_name": "example"})
def tutorial_job():
    dt = print_date()
    sleep(dt)
    templated(dt)

schedule = ScheduleDefinition(job=tutorial_job, cron_schedule="@daily")

@repository
def rewrite_repo():
    return [tutorial_job, schedule]
```
